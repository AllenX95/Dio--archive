# 琢磨一下DeepSeek未来还会发些啥

> **类型**: 文章
> **作者**: Dio-晶
> **赞同**: 646
> **评论**: 63
> **时间**: 1740898773
> **原文**: [https://zhuanlan.zhihu.com/p/27412570526](https://zhuanlan.zhihu.com/p/27412570526)

---

今天周日，DS也该休假了吧，下午了，看起来像是。。。。。。

嗨，整整一周，大气都不敢出 ：） 只能等差不多消停了才敢冒个泡。

你也知道，这周肯定不是终点 ：）

先不谈V4、R2这种砸场子的家伙，作为底下干活的人嘛总得揪着下巴琢磨琢磨，过半年DS还会从裤裆里掏出些啥奇技淫巧折腾人呢？

毫无疑问，DeepSeek之前就表示过，他比黄老板还更会把玩他那根GPU。所以，基本上这周的发布，就是几乎100%贴合着着H800的形状来干的。我还当面问过DeepSeek的几个家伙，你们这么疯狂搞H800，就没考虑过国境内那大量的H20、XX、YY的感受吗？嗯，笑而不语是吧 ：）那我也只能放飞自我了。

只要脑子没有被门夹过，都会算计着DS手上肯定不只H800，老机器留给部署实验就好了，而最新的算子、库肯定不会是基于H800的东西 ：） FP4总不能踏空对吧。但是H800毕竟是美国认证合法入境的东西，明面上谁也没法指责，而H800后面的东西就会变得敏感。喏，就像新加坡刚刚抓人什么的谣言，信则有不信则无。当然也许过半年大家都掀桌子了，都无所谓了也可能。

先猜几个点 ：）

**1、如果保持合理合法，我能想到的第一个可能性是RTX 40x0/50x0类消费卡上的优化。如果能在这个相对容易获取的消费品上弄些特别的花活出来，对川总、星际之门甚至于Nvidia自己的伤害是巨大的（来个做空）。**

- a) 50x0已经支持了FP4，虽说GDDR带宽不够，但L2容量已经扩展到96~128MB了，如果能Hack进L2的替换算法写一个FlashXXX，不好说效果就差了。若官方GDDR容量要是嫌小，深圳华强北是有那么一批心灵手巧的技工，拿着热风机就能手搓扩容的。
- b) GPUDirect RMDA/Storage，黑一黑源码迁移进40x0/50x0不是什么难事，至于互联能力弱，其实真没必要纠结，就像DeepSeekV3发布时还说推理Attention要搞TP4，到周末发的最终优化根本就没了TP4一样，最终大家需要平衡的是计算时间与通信时间的长度，两个要凑得刚刚好一样长，才是最好的计算和通信。
- c) 至于Fix Pipe，假设算白送的，搞得不好在今明年多模态的一些算法里面，把部分硬流水Hack一下搞成Flex Pipe，那部分功能是具有通用性的，跑的飞起也说不定。

**2、 FP4/6的算子是一定会有的，严格来讲这东西不完全敏感，毕竟我作为爱好者写了个5090的FP4算子，恰好能在BXX上跑那不是我有问题而是Nvidia的工程师优秀。这种算子一定会有，不过随着浮点数位宽的减少，如何控制Scaling精度就变成了皇冠级的问题，就像DeepGEMM在GitHub的描述表面上是什么1350TFlops，实际上暗藏的是控制精度的秘密（很多其他公司FP8训不出来这里关系很大）。而到了FP4/6，那什么时候保留累加尾数？保留多少？精度要求是Tile级还是要全量矩阵？A和W是否采用不同的格式？真接天莲叶无穷变了，所以后续如何搞FP4/6的算子是个大活计了，DS活菩萨肯定会发一些，可以等等看，但是除非完全照抄（含B200），否则下来还得基于硬件的微架构和手上的算法做适配。那DeepSeek还在论文中建议硬件搞什么随路量化（甚至Memory内部），(ˉ▽￣～) 切~~，就像想把那猴子压在五指山下，那满手都是猴屎好吧。**

**3、 上下文的处理，NSA肯定是有了，考虑到多模态的引入，再叠加网页搜索的各种信息，Attention的计算将会根据用户的呈现巨大的差异化特征，不在是一个简单的FlashMLA能解决了。如果，如果DS能够获得GB200的机器，使能nvlink-c2c，那盲猜KV的原始数据会存放在CPU的DDR内部，不同的KV压缩算法会充分搭配到CPU的内存和算力，即CPU和GPU会在Triton中呈现混合XPU/MEM的Flash算子。当然，也许PCIe也不是不能玩，可能会发布基于PCIe的算子，但算子的真身，却藏在不可妄言之处。**

**4、 部署策略是没有最优解的，不必盲从。其实周六发布的推理部署方式（含计费方式），也已经和V3论文上的推荐有了很大的变化。例如V3论文的推荐策略Attention是做了TP4的，到昨天那个策略，那位做TP4的兄弟就鸽了呗。不复杂，在H800上是追求最大吞吐的候，TP4对带宽的额外损伤就变得不划算了呗，但是H20、H20啊，听懂的点个赞呗 ：）114卡也是，1卡2E慢是慢了点，但通信时间那么长彼此都还得削足适履不是。除此之外，如果你有心，把每个token需要的计算量再算一下，你会发现H800的算力还有余量（说好的DeepGEMM 1350 Flops只是瞬时），还能优化。所以，DS也要去魅。**

- a) 为什么EP全并行在DeepSeekV3有那么好的收益，甚至于引发那么大的争议：） 我举得例子很简单，所谓MoE就是把全科医生搞成了专科医生，看着好像负担减轻了，但给病人带来额外的寻医压力啊。咋办，办个大医院呗，每个医生一间房，然后打广告，只要病人足够多，那任何专科医生都是满负荷的。就是个人海战术：）强行通过用户的稠密，把稀疏模型干成了稠密模型，遇到中国人又多，只要政府帮忙引流，能不赚钱么（那某网红医院亏损，病人不够是锚点）。 但是，但是，这可能是一个天时地利的甜点。如果下一代MoE1024，如果还按照EP全并行，Batch128，能快速打满128K的用户吗？ 兵无常势、水无常形。
- b) B200，这玩意儿毫无疑问会成为部署的新变量，因为他有两个Die。Nvidia肯定是希望用户把两个Die当做一个Uniform来用的。但是你肯定知道，如果当做NUMA来用，局部性能会更高，你要是DS的同学，铁定会拆Die用，而维系UMA的接口带宽，搞不好用来做点其他的某些TP更有价值。所以，下一轮的部署推荐，搞不好会有个意料之外的1/2参数，你得自己下来研究研究 ：）

嗯，如果DS菩萨还是个好心菩萨的话，我在猜他未来会越来越打偈语，一些若有若无的参数，可能会隐藏可以适配的变化空间。

周末瞎想写了这个，也不知道其他有兴趣的同学能不能也来猜测几个玩玩？都是被碾压的弱者，要团结啊。

![](./images/7e2519e74f8f0aa04839e5e0f0d36168.jpg)![](./images/7e2519e74f8f0aa04839e5e0f0d36168.jpg)

---

*由知乎爬虫生成于 2026-02-01 15:39:00*
