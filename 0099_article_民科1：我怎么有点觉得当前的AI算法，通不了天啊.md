# 民科1：我怎么有点觉得当前的AI算法，通不了天啊

> **类型**: 文章
> **作者**: Dio-晶
> **赞同**: 150
> **评论**: 26
> **时间**: 1726370491
> **原文**: [https://zhuanlan.zhihu.com/p/720274928](https://zhuanlan.zhihu.com/p/720274928)

---

额，开始写民科文了 ：） 这做技术，做着做着就开始陷入民科出不来是什么鬼。

我先得承认我从来没做过AI的算法，我就是个造铲子的硅匠。看着江湖上这火热的场面，虽说也是浑水捞了点小钱钱，但你说这人哪，总是管不住这嘴想从打硅铺里探出头来说，啊，大姐姐你这个姿势可能不对哦。。。。。。。

![](../images/8313dc9ae0b505cbec4c4dac88fcd21f.jpg)![](../images/8313dc9ae0b505cbec4c4dac88fcd21f.jpg)

---

当下的AI，泛指Transformer，它的基本原理和算法，其实差不多都已经在白盒化了。Transformer用多层FFN高纬空间表达非时空的结构，而用Attention表达时间和空间的相关力场，也可以说前者是Stateless后者是Stateful。

Attention的问题在于时空力场，时空自有时空的民科，在HPC里面时间是最难解决的（Memory Bound就是一个表征），有空了另写一篇民科讲Attention。

今天我民科的是这个时空力场无关，纯高纬空间结构部分。

其实我们也可以把这个空间结构理解为Knowledge（Knowledge本身具有时空不变性），算法上就是把非结构化的Knowledge量化为高纬空间的点，嗯，只要维度够多，世间万物都可以被区分开，并用一个多维坐标系来表示，然后再用几何距离的算法对坐标向量进行相似性计算搜索最优解，其实和过去数据库、特征分析机理差不太多，但是当前的算法采用统计拟合的方法，也就是学习，获得了远超传统算法的效果。

当然，我们会纠结于是否自然语言不能代表物理世界的全部，而通过视觉等更多的数据输入有可能建造更庞大的World Model。本质不变，作为一种有效的对世间Knowledge可计量的算法，可以说它是一种智能的载体，具有无尽的潜力和可能性。

正如

[Dio-晶：鄙视AI，质疑AI，理解AI，完成AI！](https://zhuanlan.zhihu.com/p/687281753)

贴子所述，我作为一个造铲子的硅匠，也能深刻地理解并认同AI算法那令人充满梦想的可能性，是真的有金子，甚至于，得顶礼膜拜，搞得不好这更是一条通天之路。

![](../images/38f8ae42e3365d9af96092d3f842ad61.jpg)![](data:image/svg+xml;utf8,<svg%20xmlns='http://www.w3.org/2000/svg'%20width='414'%20height='486'></svg>)

根据《圣经·旧约·创世记》篇章记载，当时人类在示拿之地联合起来兴建希望能通往天堂的高塔；为了阻止人类的计划，上帝让人类说不同的语言，使人类相互之间不能沟通，计划因此失败，人类自此各散东西。此事件为世上出现不同语言和种族提供解释。而东方也有类似的记载，《山海经》不周山。

> **但作为造铲的硅匠，我要硬说铲子本身（含Nvidia）有点不对劲呢？**

随着对AI算法和计算范式未来演变的深入思考，让我感觉在整个AI算法在非常基础的底座上有破绽（基于我那个[Dio-晶：鄙视AI，质疑AI，理解AI，完成AI！](https://zhuanlan.zhihu.com/p/687281753)帖子对范式的判断），从源头开始错，几乎让我肉眼可见的认为，当前这算法，逆生三重，它通不了天啊。

你肯定不信，我也是，因为在数学上这算是挺简单的错误，我实在没法相信那么多聪明的数学家，还有计算理论的没有发现这个问题。本来我不敢乱讲，觉得太民科了，所以是借着在部门讲座的机会，把问题写成故事藏在其中捋了一遍，发现讲的逻辑上也没问题啊。这让我实在不知道为什么我能看到这么严重但是简单的问题到底是错在了什么地方。开始陷入民科，所以本文把我觉得最大的破绽列出来了。

---

当我们所有人都在品鉴一颗GPU是800TFlops、1P Flops算力时，这个Flops，它的计量单位，大多数人理解的可能都是乘法，或者融合乘加（fused multiply-add-FMA），你看下面这位大神的原话。

![](../images/86c71adc77f53d8fae3fd8810a8a5172.jpg)![](../images/86c71adc77f53d8fae3fd8810a8a5172.jpg)[Kimi创始人杨植麟最新分享：关于OpenAI o1新范式的深度思考](http://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMjc1NjM3MjY2MA%3D%3D%26mid%3D2691550914%26idx%3D1%26sn%3D01c8caf300548189720ca16b764cf5a9%26chksm%3Da8be383884fea5a50ce1ea86de3ba6231fada1706d08472afbd88d8485e053cae7cfbf9eb4a1%26mpshare%3D1%26scene%3D24%26srcid%3D09148UqrF9YVzKMDtI79SKbE%26sharer_shareinfo%3D2d58c080651cb8ac80c1363c712f6204%26sharer_shareinfo_first%3D8fe272adf60182bbc80bd7ba4feba894%26ascene%3D14%26devicetype%3Dandroid-31%26version%3D2800325b%26nettype%3DWIFI%26abtest_cookie%3DAAACAA%253D%253D%26lang%3Dzh_CN%26countrycode%3DCN%26exportkey%3Dn_ChQIAhIQubZ7SO9FclgoSsScQdOxWBL0AQIE97dBBAEAAAAAAHk6M%252Fd3BaQAAAAOpnltbLcz9gKNyK89dVj0sKEkfEYENA3Gd%252FxMyRtAbuj9n7lr%252F6qUW2NTZel%252BGpCoPJ04AacFnfmDhdqGtjh3LNJ%252FsDUm87wzzx0xmugySyRY5Iqmz%252FXKrTzm%252BgLCcFxP7TcTpkkzz59GkewOV28RdIPtmC8b0%252FLnvNGI43xH%252BewYX8XjhM%252FWZ5zVEVQFEfEV0KIC16oXCRgfDOyYg8gjemWwkyPUKwyjcWYc6aSrWXGzFtceypfnn5f6tGzmry%252FlZsI3vHNecnMfuCfZRoC1ygWSR4T1j8OV1vTnWMc%253D%26pass_ticket%3DSkeWO3WUbZhKMIRNED8fvN9kQD%252BNKopnn5QVEmi9H3tuYcPFA2Zv4lDFt0k6Npl5%26wx_header%3D3)
> **但实际上，当前所有ai芯片的Flop仅仅是指Blas3的GEMM算力（这货的计量单位其实不是乘法），根本就不能按Flop算。**

其实原本大家谈算力都是HPC算力，Nvidia最初设计GPGPU也都是为HPC设计的。这个算力也并不仅仅是Scalar的FMA，而是BLAS算力（Basic Linear Algebra Subprograms），包括Blas1、Blas2、Blas3，简单说明的话，Blas1包括向量与向量的很多计算算子，Blas2就塌陷为向量与矩阵的几个算子了，到Blas3则完全收敛为矩阵和矩阵的单一内积算子。

但到了AI的时代，主要算力（90%）几乎就是Blas3的GEMM，大家讲算力的时候，默认都是收敛为单一矩阵内积乘的算力了。GEMM是一个非常单调的算子，矩阵x矩阵原本并无意义，更多时候是Parallel的GEMV或DOT（内积）。如下蓝黄相乘，消耗了1次8 Flops的内积算力得到Cij，这个算力的是一个最小粒度的向量内积算力，你并不能把他拆分成8次1 Flops的FMA算力。

![](../images/20e22e5020fc974dc6fd9903ff35659f.jpg)![](../images/20e22e5020fc974dc6fd9903ff35659f.jpg)
> **DOT，即Distance = Vector \* Vector，两个向量对应位相乘再求和的内积计算，在N维欧式空间中，他代表A点与B点坐标的夹角和投影距离。几乎整个AI算法都是在用这个算子来求解某个欧式空间中两个Embedding（即Knowledge）之间的距离，通常认为夹角代表相似度，模长代表重要性。**

问题就在这里，内积是欧式空间的特征，所以AI Embedding所在的高纬空间，要么就是个高纬欧式空间，要么算是由若干局部欧式空间拼接成的高纬非欧式空间。

想一想这个答案也很好笑，人类所有的计算芯片，数十年在计算机体系结构攀爬科技树的结果，最终在CMOS管上能够提供的最高效的计算能力，固定为**DOT= Vector \* Vector内积，**当然，你也可以选择做一些其他距离计算，例如汉明距、余弦距、甚至更简单的曼哈顿距等等，毫无意外，内积距之外任何算力，在Nvidia GPU也罢、Google的TPU或者菊花的昇腾上，都会不足标称算力的1%。。。。。。。当代的计算机，看着就像一个只有加法键的计算器。

甚至于，就如我喝多了的狂言，即使是面对面一位菲尔兹奖的数学家，我也丝毫不虚，无论你在黑板上写出什么我看不懂的公式，但凡你有了想借助计算机充沛的算力来推演的念头，OK，你大概率就会拉到内积计算的层次，然后被我用丰富的内积打硅经验战胜。

欧式空间源自空间几何，是人类对三维时空抽象建立的数学模型，这个模型在不涉及相对论的情况下（大多数HPC）这个空间是相对真实的。但是，**AI算法本想要一个高纬空间，却因为只有内积的原因只能用欧式空间去构建，而高纬欧式空间在数学上是有基本问题的，我有时候觉得，物理上根本就不应该存在高纬欧式空间。**

---

维度灾难，所有涉及聚类算法的人都应该知道的基础概念。维数灾难（英语：Curse of dimensionality，又名维度的诅咒）是一个最早由美国应用数学家理查德贝尔曼在考虑优化问题时首次提出来的术语，用来描述当（数学）空间维度增加时，分析和组织高纬空间（通常有成百上千维），因体积指数增加而遇到各种问题场景。这样的难题在低维空间中不会遇到，如物理空间（大多数HPC）通常只用三维来建模。

用基础几何就能理解的问题，很简单。

- 一个N维正方体，以及其中的内接球，即球的直径与正方体边长相等（半径1/2）
- 3维，正方体边长为1，那么其体积为1，而内接球体积为**4/3\*π\*r³ ~= 0.52**
- 8维，正方体边长为1，可算出体积为1，内接球体积是**4.05\*r^8 ~= 0.016**
- N维，正方体边长为1，其体积还是为1，而内接球体积**K\*π\*r^n**
- 很反常识，就是球的体积解决为0，这代表N维正方体空间内中所有的坐标点都会聚集在球外，即高纬欧式空间中，内积计算的模长意义会消失，即高纬空间下，所有点的距离是相等的
- **就像人类在地球仰望星空，所有的星星只有夹角，没有距离、没有时间，所有的星座都是虚假的，这大致就是AI的幻觉的源头吧**

维度灾难在高纬内积下是真实存在的，典型的例子就是向量数据库（想要以存代算），而最终，主流算法是Hierarchical Navigable Small Worlds (HNSW)，图搜索，这相比内积直接算距离不仅需要耗费更多的算力，而且还存在大量的串行计算，即消耗的时间，是内积的万万倍。

解决维度灾难的聪明人和算法很多

- 最简单的莫过于Dropout、数据分类等等，这都是改善，不能根治。
- 也有人放弃模长，只计算余弦相似度，因为模长往往用于表达单词的频次，这会导致信息量丢失，但更麻烦的是，放弃模长的的欧式空间余弦距计算的计算复杂度远大于内积。其实Softmax就是一个余弦距计算，做过的都知道，其消耗的算力是内积的~10倍而能够使能的算力只有内积的~1/10，差了两个数量级。

![](../images/7715aa7f13c921ad6eced68dd4ee026f.jpg)![](../images/7715aa7f13c921ad6eced68dd4ee026f.jpg)

- 嗯，数学家的你，手上也许有更好的非欧式空间定义，和更好的向量相似性算法，但是，很扯淡，我当前能给你的铲子就是内积，便宜管饱，其他的？保证又贵又吃不饱。

---

打开标准的Transformer来看 ：）

Attention是时空力场（LLM只有时，Version存在空间力场），而FFN则是空间结构，并且代表了基于Knowledge的静态高纬字典，也有人用图书馆来比喻。

我理解整个FFN（大图书馆）也许是一个非欧空间、非线性空间，但正如HPC中很多从头算的问题的化简一样，它被拆解为局部的欧式空间粒度，全局的非线性也可以通过多层线性叠加层间非线性拟合。但打开看，每层FFN内部（每个书柜），是内积距，就是个欧式空间。

典型，词向量的Embedding是512维的，而FFN则是2048维。FC层将输入词向量的512维升维到2048维，随后通过第二个FC层再将2048维降维回512维完成一层FFN的查询。

其实从Bert开始，词向量的维度就是300~700左右，网上有一个熵编码理论来的，好像是n >8.3K loge（N），N是词表大小，n是向量维度。英语的单词数量大约是100万，考虑到各种语言及语义的变化可能按1000万算的话，大约维度就是200左右。

- 所以NLP用512维的Embedding表达，也许是容量足够的，而单层FFN的2048维，也许大致上也是不会出现维度灾难而实验出来的某种边界？
- 如果FFN的维度增大到4096，会有更多的幻觉？而如果减小到1024，是否又会大幅降低准确性？是否因为2048的锁定，才导致FFN的更复杂串联和并联？
- 如果LLM诞生于人类语言与内积算子恰到好处的匹配（神选），那么多模态的FFN怎么办？

问号很多，答案不得而知，都是民科猜测，毕竟我没真正上手，并不理解这一层级的实践。。。。。。。

而也许这些扯淡都是在杞人忧天罢了，就算内积，草台班子手搓手搓，说不准明年大模型又突破到新的层次。

我记得在什么地方看到一个文章，说是AI网络的维度和精度是可以相互互换（感觉上连接关系、数据维度、数据精度都能互换），即当前精度和维度的LLM是可以用1bit精度但更复杂维度的网络等价的。1bit的网络看上去会更接近人脑，但是内积的算子，就怎么看都是不可能训练出这样的网络的啊。

Yan LeCun说LLM是一条邪路，真正的AGI只会诞生于脱离文字的World Model，我猜，用熵编码理论来看，从Text到Vision，维度的数量怎么也不会是500级别了吧，要到更大的维度空间，5000、50000，内积算力还能用吗？

**我不知道我整个逻辑算错了什么，反正我就是个硅匠瞎BB罢了，无论你对AI有什么美好的梦想和愿望，你得找我或和我相似的硅匠打铲子，而硅匠当前打得顺手的，就是这么个内积铲，我觉得有毛病，你觉得没毛病，那可能就是没毛病。。。。。。。**

---

写在最后：

其实整个文章的逻辑是反着写的 ：） 我理解Transformer的Scaling Law，主要的增长尺度就是FFN，而FFN的增大不外乎串联更多层、并联更多层、每层变大三种变数。串联更深的层次的收益在降低、难度也在增大，并联更多层次也就是MoE，这玩意儿训练起来稳定性很差（我有一个关于MoE稳定性无解的民科，有空写），剩下就是增加每层FFN的本体层次，这个尺寸在我的理解下，因为内积算法触发的维度灾难问题，也无法增加了。。。。。

**再向前，可能通不了天啊。**

---

*由知乎爬虫生成于 2026-02-01 15:39:00*
