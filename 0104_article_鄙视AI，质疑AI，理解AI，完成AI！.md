# 鄙视AI，质疑AI，理解AI，完成AI！

> **类型**: 文章
> **作者**: Dio-晶
> **赞同**: 555
> **评论**: 55
> **时间**: 1710572251
> **原文**: [https://zhuanlan.zhihu.com/p/687281753](https://zhuanlan.zhihu.com/p/687281753)

---

最近正方反方吵得挺火的，虽说被要求禁言了，但忍不住有好事的微友贴脸追问，实在是又管不住嘴了，就还是想凑上来接一下，总不能让这话掉地上不是。

![](../images/77735c2d9797b0604fc70c279d73c3b1.jpg)![](../images/77735c2d9797b0604fc70c279d73c3b1.jpg)

就算，最后被批评言行幼稚，感觉还是比被表扬老成稳重还是好一点啊。

嗯，我的观点，确实有些不同。

有一本书，恰恰好是OpenAI的研究员写的，《为什么伟大不能被计划》。其实这个书的名字就已经部分回答了这个问题。不过这是西方的思维方式，我并不喜欢，要用中国话来表达的话，那就得引出每一个中国人灵魂深处那最深的烙印，也是西方国家特别是美国所缺失的视角，就是历史观。就如所有的中国人都默认两岸必然统一一样，如果不是今天明天，那就是后天或者大后天。

[“尔曹身与名俱灭，不废江河万古流。”，这就是我对AI的答案，AI是历史的必然](http://link.zhihu.com/?target=https%3A//m.gushiwen.cn/mingju/juv_44a8d32b1a83.aspx)

---

人，面对世间的自然万物，不知者若恒河沙不可计数。

> **而计算的本质，就是对事物的求解。我有一个问题，我希望得到答案。y=F(x），输入问题x，寻求答案y。**

古人用蓍草和龟甲来占卜，其实和现代人用计算机求解是一样的，没必要用鄙视链来看待古人，计算机也就是正确率高一些罢了，计算机也会出错的。古人也已经在尽全力（甚至人祭）来改造他们的计算工具了，文明程度不一样，但大家追求的是一样的（想到这也就不要苛责AI的幻觉了），反正，就是尽可能算出一个更接近真实的答案。

计算的发展史都只是在寻求一个更加高效、更加准确的求解方法，你看即使现在，我们开启一个大模型训练之前不还得烧个香求收敛求平安不是，某些求解方法那都是代代传承的。

其实即使人们学会用计算机加速计算的求解，即从图灵机的诞生至今，也还未超过百年，都只是文明的一瞬罢了。。。。。。。

甚至于在某次会议，一位领导问在场的做计算行业的一些同学，“不算Mobile，用过X86指令集以外的CPU的人举手”。很难堪，举手都相当少。历史并不长，人生苦短。

要知道图灵机的大规模民用化，即X86（Intel）真正的崛起至今都还不超过30年 ：） 嗯，会议2002年我读研究生做项目时的合作公司里，那最宝贵的资产，就不是X86，而是几台基于SPARC指令集的SUN Work Station，那巨大轰鸣的机器占据了办公室正中心，至今记忆尤深，当然还有配备的全职维护的一位开马6的气质美女姐姐。。。。。。

![](../images/150ba520f20b918fe3647de591f8cabf.jpg)![](../images/150ba520f20b918fe3647de591f8cabf.jpg)

难不成我在计算的历史已经算活得很长了？虽然我也只能从~2000年开始记录我看到的计算史，即使我把我看到的历史列举出来，也只能看到一切皆为定数。

要回到2000年左右，这个世界的各个计算机领域甚至都还是Domain Specific的，彼时的SUN公司可比当下的Nvidia还更如日中天，IBM的Blue Gene还统治着HPC，而Data Base则更是一套复杂而昂贵的System。

但开源和云化来了，基于Domain的城墙被推倒了，数据平权让数据计算变得平民化，而接下来的Iphone时刻让数十亿个体都成为了廉价的人肉数据采集器，大量数据触发的数据爆炸迫使了以Google的三驾马车为基础理论的分布式数据库和大数据迅速形成了巨大的数据容器，此时的人们开始意识到数据和石油一样蕴含了价值，类似于炼油厂的互联网公司诞生了，但那仅仅是汽油和柴油，而再其后面的AI才是对石油更深度浓缩提炼，诞生的橡胶、化妆品、药物…….

狭义的数据库是指存放数据的仓库，就像石油的储油罐。但广义来看，整个计算的本质是围绕着数据的一整套泛数据库的粗加工&精加工的产业链，计算产业和石化产业并无二致。

这近廿年的计算范式演进史，就是随着数据的量变，而必然选择的基于更多数据量的更高效计算范式变化史。

而且，计算的演进也是符合人类本身的演进诉求的，也就是马斯洛金字塔。所谓饱暖思淫欲，在满足次一级需求之后会逐渐追求更高一级的需求，而高一级的需求相比次一级需求具有更高阶更精华的Hierarchy。

---

> OpenAI 首席科学家 Ilya Sutskever 在专注于计算理论研究的 Simons Institute 作了一次讲座，总结就是：**AI大模型的本质是压缩。**

这并不容易理解，但其实也很好理解。

万事万物都具有相关性，除了部分网民，大家都能理解关系是一个多维度的事物相关性而不是非黑即白。那么所谓的AI即压缩，就是基于微积分+范畴论+几何代数，不断计算数据之间的Relational Distance，不断寻找各向最短距离，然后进行浓缩信息的过程。

就像给你一堆汉字：鲫、鲸、鲶、鳜、鳗、鲅、鳉、鲑、鲱、鲥、鳎、鲉、鲬、鲿、鳢、鳐、鲷，肯定有字你不认识，但你心中一默念，进行一次分类，这都是鱼！ 这就对了，恭喜你完成了一次压缩（分类也是压缩）。再举一个视觉上的例子，下图是在二维空间的通过欧氏距离计算得到了全局点的二分类，这种基于欧式距离的维度划分叫做聚类，即小球在位置分布上的信息量得到了一次压缩。

![](../images/ff8f360e222c3102019709f177a2449f.jpg)![](../images/ff8f360e222c3102019709f177a2449f.jpg)

压缩是一个比较中性的描述，也可以理解为抽象（但是抽象往往带着有损的含义），也可以理解为分类（但分类总存在划分不够多的情况）。Anyway，其实就是寻找共性，去除冗余的一个过程。

甚至有时候，我们会觉得Transformer有点像傅里叶变换，其实也没没错，下图也是一个例子，复杂时域信息转换到频域，连续复杂的时域信息能被频域仅用两个尖峰就表达了，这就是压缩的过程。

![](../images/76cd82b0bab72bd7e0455bbd98bc4d13.jpg)![](../images/76cd82b0bab72bd7e0455bbd98bc4d13.jpg)
> 追求用更少的能量来承载熵减，就是压缩，这也是文明的本质。

压缩并不是AI的专利，而是一个随着数据源扩大后正常的算法演进过程，如下图。数据最初爆发时的压缩算法是传统数据库，是的，传统数据库本质也是压缩，而Transformer也就是一个高纬数据库。很多人认为于搜推广和AI大模型是两个不同的物种，但我始终认为他们都是一条路径上不同阶段的产物，类似是原油在石化产业中不同阶段的产物。

![](../images/ce0eb0e59b2f9f9c425ba4f0cfb12215.jpg)![](../images/ce0eb0e59b2f9f9c425ba4f0cfb12215.jpg)

- 传统数据库的数学方法是Category，建立索引归类，数据本身不变，并且显性地按表空间类别，例如男女、年龄、身高分门别类地放在各自的维度，支持插入、删除、等功能，相同的信息（例如两个人同年龄）可以随着重复的归类而信息压缩；
- 搜推广也就是电商时代，是随着商品细分类型的急剧增大，并且涉及到人的细节特征时，传统数据库的显性分类就变得特别麻烦，例如一个二次元可能喜欢原神，但是他喜欢海贼王吗？这就很难像年龄身高那样归类了。所以如果显性分类已经做不到，搜推广算法就引入了几何代数+AI，把分类的方法从显性变成隐形，就是我不知道用户到底喜欢什么，但可以假定把用户信息库和商品库用可以被一个百维映射隐空间进行映射（放到一个巨大的Storage里），具体映射关系则让AI通过训练得到，即用Differentiable（可微）方式拟合用户和商品的关系，再把数据Embedding到一个高纬隐空间数据库（其分类已经无法直观理解）中来表达。 搜推广数据库会随着商品和人的数量及映射复杂度而增大，但其中商品还是商品，人还是人，即红内裤和白内裤都还是内裤，只是每个商品和对应的人都被数字化打上了标签，而这个标签在多维的隐空间中被映射成了相互之间的关系，即红内裤和12/24岁的人之间莫名其妙地显得空间距离最短。
- LLM即再面对Native Language时，其背后是再一次对Native World的宏大的Knowledge进行的压缩，那搜推广的人与货这种百维映射就又显得完全不够表达了，所以LLM引入了高纬Unstructured数据结构+Scaling-Law，先假定Knowledge的维度是万维（12288），并放弃把Knowledge放在Storage而是直接存放在Networking内部，用一个万维的隐空间来表达知识的映射。此时的红内裤就不再是内裤，而形成了一种本命年得红屁股的精神象征被承载了，嗯，即使再过去几百年，人们早已不穿内裤了，AI还是会提醒你本命年最好把屁股涂红，这条Rule还在。

是的，AI大模型的压缩，我理解是基于一个万维隐空间的坐标映射关系。作为三维世界的生物，也许很难想象万维空间是什么样，要从视觉上感受的话，就是夜空里的圆顶星穹了，星空是典型的高纬空间在低纬上的投影的样子。

![](../images/8c26fe43a846cbb8cb7bcefd6b3be44e.jpg)![](../images/8c26fe43a846cbb8cb7bcefd6b3be44e.jpg)

AI大模型压缩的过程，每一个Raw Data都会经历一系列域变换，变成高纬空间中的一个坐标点（Token），然后Embedding（中文的嵌入也非常形象）到高纬空间的一个位置上。要方便理解的话，还是把它想象为夜晚的星空吧，当你输入“[大海啊，全是水，河马啊，四条腿”](http://link.zhihu.com/?target=https%3A//zhidao.baidu.com/index/%3Fword%3D%25E5%25A4%25A7%25E6%25B5%25B7%25E5%2595%258A%25E5%2585%25A8%25E6%2598%25AF%25E6%25B0%25B4%25E6%25B2%25B3%25E9%25A9%25AC%25E5%2595%258A%25E5%259B%259B%25E6%259D%25A1%25E8%2585%25BF%25E4%25BB%2580%25E4%25B9%2588%25E6%2584%258F%25E6%2580%259D%26from%3Dqb%26samplow_val%3D703)这种废话来训练大模型的时候，这四组词就被翻译成四组坐标（Encode），你会看到夜空中有四颗星星相继变得更加明亮，它们之间隐隐约约的连接关系也被勾勒出来。下次写文章的时候，如果触发了某颗星星，这句名言就作为其中一个星座隐隐约约显现出来（Decode）。所以LLM大模型并不是玩文字接龙，他是在万维空间的星空坐标及其相互关系中寻找的某条航路。

万维夜空的星系坐标和关系都是被训练出来的，就像你无法解释宇宙一样。其含义也非常隐晦，而且星星会因为新的信息带来而位置和关系的变化，就像你在装修自家的房屋，要把一幅达芬奇的挂画挂到墙上最合适的位置，然后，每一位家人都会给你一些Hint，你听从他们的意见不断上下左右移动，直到最表达出尽可能让大家满意的美，虽然你也不知道是不是最美。

并不是所有的信息都适合万维隐空间表达，世间也有很多处于基础规则（数学物理）的有限维固定表达，例如九九乘法表，算是一个固定81维（这个维度大致和人类选择十进制，以及十进制可能符合人脑某个甜点有关），任何小学数学题目，就像3/4+7/11\*25/26=？都可以被表达为九九乘法表在其维度上的若干路径，这种映射关系是稳定且不变的，当前AI大模型反而因为维度太高，反而对这种固定低纬表达存在幻觉（常常算不对算术）。这个问题后续是需要算法解决的，如果类似九九乘法表、宏观世界的牛顿定律，都应当作为网络结构的某个定式，就像人马儿座、仙女儿座一样固化，这一方面避免幻觉，一方面又能加快训练学习过程，还能传承，嗯，AI相比人类缺的就是老师这个圣职，当然这又回到隐空间的可解释性问题。

所以，AI大模型的压缩，本质上是在追求某种Lossless压缩器，以高纬的结构和关系的一种数据库，世间所有的知识都存在万维的收敛域，即可以在其中被存放和查询到。这些万维坐标之间的结构和关系，就是智慧了。

要还是不好理解的话，学佛的朋友可以再熟读几遍金刚经

**【须菩提。如恒河中所有沙数。如是沙等恒河。于意云何。是诸恒河沙。宁为多不。须菩提言。甚多。世尊。但诸恒河尚多无数。何况其沙。须菩提。我今实言告汝。若有善男子。善女人。以七宝满尔所恒河沙数三千大千世界。以用布施。得福多不。须菩提言。甚多。世尊。佛告须菩提。若善男子。善女人。于此经中。乃至受持四句偈等。为他人说。而此福德。胜前福德。】**

**【须菩提，我今实言告汝。】**

**【若有善男子。善女人。以七宝满尔所恒河沙数三千大千世界。以用布施。得福多不。】**

**【佛告须菩提。若善男子。善女人。于此经中。乃至受持四句偈等。为他人说。而此福德。胜前福德。】**

佛教为什么认为金刚经为何有超越恒河沙数大千世界的福报，不就是压缩得足够精妙么。文明、宗教、AI都是在尝试寻求世间的金刚经，一个Knowledge的最小载体，承载熵减所需要的能量越小，越本质，越能长存。

从这个角度看AI既适合无神论者，也适合有神论者。

- 对于有神论者来讲，神用了七天创造世间，AI也许就是在寻求初始那七天的圣言；
- 对于无神论者来讲，世界也许本质是大统一论下的随机分形结果，AI不断的压缩反分形本身就是在寻求最初始统一理论罢了。

---

AI是怎么计算压缩的？

其实AI走到今天，很多迷雾早就揭开了，NN并不是一个无法证明的黑盒。虽然没有达成一致，但很多专家教授都给出了各自的AI理论的第一性原理，AI具有数学可解释性基本上是已经达成了共识的。

额，我大学专业是通信，高等数学肯定不咋地。但我高中的数学还是蛮好地，嗯，我高考的物理是满分，数学只错了一题（和创造历史擦肩而过），所以我也喜欢用高中数学的范畴来理解AI。

我认为：

> **“AI是一种新的应用数学（或者说计算范式）。如果是几何是人类对物质世界的结构的数学表达，物理是人类对物质世界的关系的数学表达，那么AI是人类对知识的结构和关系的数学表达，AI把不可计数的知识变成了数学可计算，如恒河沙般不可尽数的知识，可计算了，可数了** **：） ”**

嗯，我是一直搞不懂为啥几何是数学的一部分，严格的数学应该就只有逻辑和集合，其他都是某个问题的数学化求解，即解决问题的应用数学。几何是空间（结构）的应用数学，物理是力（关系）的应用数学，所以AI也就是Knowledge的结构和关系的应用数学，回到根本还应该是逻辑和集合。

哈哈，如下所写的一切很可能不对，只是一个朴素高中生的数学观。

那么，AI的压缩空间从何而来呢？

先看表面，Native Language也是一种Knowledge的数字化（也可以说符号化）的压缩，但是其表达方式是受限于人类的生理结构而低效的。语音需要符合人听觉和发音的频段，语义（包括文本）需要满足大脑的input Bandwidth，机器没有这个约束，正如我写的这段话，也许可以用一个复杂的符号即可表达，就像神话中的符文、道士画的符。。。。。。

再看内里，人与人、人与物、物与物之间的隐规律，即使数据还是那些数据，并没有突破人类的边界，但AI能够用上帝视角来整理分类，搞得不好就能得出个哥德巴赫第二猜想，也许无法证明（隐空间，AI也讲不出来），但能够从足够的数据中的压缩中得到一个隐形的规则。

然后，如何计算呢？

寻找有限域呗。有限域是包含有限个元素的域并且存在加减乘除等运算完备运算规则的集合。如果我们喜欢看玄幻小说，都应该记得，“**时间为尊，空间为王，命运不出，因果称皇。**”这就是玄幻域的完备计算规则 ：）如果表空间存在可压缩的冗余，那么就可以寻找到一个更小的隐空间有限域来做域映射呗，当然，隐空间越大，映射的知识越多，也就是Scaling-Law。

AI就是寻找一个完备但更精炼的隐空间来表征表空间的过程。

再接下来，其实就回到味道熟系的HPC领域了。。。。。。AI在计算上各种技巧和取舍，在HPC都是可解释的（要说是玩剩的容易伤人），都是来来回回几十年的那几招：Blas1、Blas2、Blas3、Parallel、Serial（Sequence/Divergence）。

![](../images/c23dceaff813b7e72a9cd49690409394.jpg)![](../images/c23dceaff813b7e72a9cd49690409394.jpg)

- Blas1，典型的AXPY，Scalar = Vector \* Vector（向量乘以向量）就是在有限域内的Distance运算，典型的AXPY就是个欧式距离计算，还有很多非欧式距离算法，包括海明距、余弦等等，距离短即相关度高，距离远即相关度低，聚类就是调用这个东西。
- Blas2，典型的GEMV，Vector = Vector \* Matrix（向量乘以矩阵获得另一个向量），就是一个域变换，Encode、Decode，把A空间的Token转换到B空间的Token。
- Blas2，如果Blas1 + Parallel，即并行计算多个Distance，也是Blas2。
- Blas3，就是GEMM了，Matrix = Matrix \* Matrix（矩阵乘以矩阵），这也算一种域变换，不过是Graphic的，即把Graphic通过某种Pattern识别，典型CNN卷积，转换成Vector（Token或者Vector再转换成Token）。
- Blas3，同样，如果如果Blas2 + Parallel，即并行计算多个Text的域变换，也是Blas3。

所以，所有AI网络的计算差不多我都能用我的朴素数学观来解释。CNN是卷积，变换一下就是Blas3，所以对矩阵算力需求很大，LLM其实主要是GEMV，即对向量做域变换，因为Blas2的Flops/Byte偏低，所以LLM推理会体现为Memory Bound，而LLM训练则因为每次有多份数据batch在一起，即形成了Parallel+Blas2，就变成了Blas3，Compute Bound了。Blas1的Flops/Byte极低，所以只能间差在其他计算之间或者Parallel成Blas2实现（K-means）。

至于Serial，有人说AI只有Sequence没有Divergence，哈哈，但我有一个相对比较大胆的解释：Softmax就是Parallel的Divergence，AI Neural Network拟合的y=(x)是一个线性函数 ：）

算是暴论吧，我们通常都把relu、softmax定义为非线性的，所以神经网络呈现为分片线性函数关系，每一层之间是独立的线性函数。在推理的过程中是的，但是在训练的过程中，在我们寻找整个星空魔法的位置和关系时，softmax更像一个Router，一个大自然分形器的作用，如果训练的数据是巨大且饱和的，那么Softmax的Router就是遍历的，最终整个F(x)的拟合过程是连续函数。

就像，你掏出地球仪寻找企鹅，先找到南极？ 额，不对，Softmax排序的结果是找到中国，放大，找到深圳市，放大，找到南山区，放大，找到深南大道和南海大道的交界。整个搜寻过程是分片函数，是非线性的，但是并不妨碍完整地球仪是线性的。

我认为把Serial的Divergence隐藏到Parallel中，降低Serial的浓度，就是AI这十年成功的关键Trick。

我挺喜欢最近OpenAI员工号称每日必读的《The Bitter Lesson》，[https://www.jiqizhixin.com/articles/2024-02-23-2](http://link.zhihu.com/?target=https%3A//www.jiqizhixin.com/articles/2024-02-23-2)，但我觉得很多不做芯片的算法工程师还是没读懂 ：） 什么是随着算力增长的通用方法？ 其实在HPC领域这事早已归纳过无数次了，就是Blas1、Blas2、Blas3、Parallel、Serial，在内存带宽不足时更多用Blas3（Tensor）替代Blas2，在单线程难以为继时用Parallel替代Serial（GPU替代CPU），就是整个AI发展历史的各个Milestone的关键。

其实，如果回归到Blas1、Blas2、Blas3、Parallel、Serial来看待计算的通用方法，那所谓Cuda是生态、城墙、或是沼泽，都不重要了。

你看Transformer为什么成功，是因为Cuda吗？

不不不，它就是《The Bitter Lesson》的一个典型案例。

即使人的语言是Timing Sequence的，但Transformer还是采用了Parallel替代了Serial，用多个Attention之间的全相联Relation替代了Sequence的Timing Relation，从而跨越了传统LSTM在单线程Serial Sequence的泥沼，再叠加Softmax用Parallel替代Serial的Divergence，就展现了All your Need的碾压力。

但反过来看，按照我这套暴论，我就不太看好Xshot的路径了，Long Context表面上是很美，每个人每天的语言上下文加起来大概也就10K的Token，人的一生也就20K Day，简单一乘，只要把Context增长到200M尺度，人这这种动物的一生差不多就可以被AI表达了。

但是，Long Context这种行为，实际上又在Parallel之上回归了LSTM的Serial先进，如果只是KV-Cache结构，Bitter Lesson Again。Mamba做了些压缩，但只能算雕花，还是得找到一种真正把Serial无损转换为Parallel的算法，才是Long Context真的突破。啊，不过，不得不说，这也算是GPT之外的一个新的可突破维度吧。

---

> 所以，如果站在历史的长河来看，AGI是人类路漫漫其修远兮，吾将上下而求索中非常自然和协调的一步，没什么值得焦虑和犹豫的。

不过，这条路径上，确实还盘踞着若干的绊脚石，至少在我个人的心中也抱存着几个AI的基本问题还没有看到答案。

第一个核心问题是AI这个词，Artificial Intelligence，其中Artificial这个词，当前是没有给出明确的解释的：） 人造，人造，人造，认真想一想人造这个词，这个词本身就具有目标性，如果深入思考一下，它只会走向两个分支：***The One*** or ***Infinite***。

which one is your answer？ 在这个岔路口的选择当下我认为是非常关键的。

这其中的Bitter Lesson，得再进一步，从Blas走到Circuit才能找到答案了。

何为射频电路、何为模拟电路、何为数字电路？

答：射频电路是基于基础电磁场的场效应工作的电路，而模拟电路则是通过Design Rule控制住场效应访问，让电路表现为基础的R/L/C（阻/感/容）特征的连续信号，而数字电路，则是在模拟电路上再进一步，通过对信号进行区间截断，通过区间包容P/V/T（工艺/电压/温度）差异后的确定性的开关电路，表现为0/1的数字化特征，这也是二进制的起始。

为什么说到这个，from场电路 to 模拟电路to 数字电路，依次增长的是可靠性和可复制性（可复制算是可靠的衍生物），而依次降低的是计算能效，很简单，增加熵减的容错性是需要付出额外的能量的。

**人脑是典型的模拟电路，AI当前是数字电路。**

> The One

如果Artificial的答案走向The One，那就是AI造神之路，巴别塔，无需质疑，OpenAI的狂热者们的AGI梦想就是造神（开源的Llama就不是，The One是不存在被复制的，被复制的必然还不是The One），OpenAI想造神，所以才有Altman对Scaling-Law执着以及7万亿美元的追求。但是呢，回到自然界，如果尺度增大就是王道，为啥人类没有进化为泰坦？为何陆地上最大的动物仅限于大象的尺寸？能耗和容错就是其中的关键点。随着尺度的增长，能量消耗急剧增长，MTBF（容错性）急剧下降。

![](../images/c711cfb6c6a34a535ea7c0335e223671.jpg)![](../images/c711cfb6c6a34a535ea7c0335e223671.jpg)

Meta用了5万张H100来训练Llama3，如果再增长到T级别的参数规模，需要的能量是什么？而至于容错，额，我还在2021年写过一篇帖子，<https://zhuanlan.zhihu.com/p/383074778>，AI大模型的训练已经不再是分布式计算了，而是多机单任务紧耦合了，并且随着系统扩大，单个失效造成的错误的代价也在增长。两者是矛盾的。是付出更大的能量来增加系统的可靠性（这个付出的能量是规模的高阶曲线），还是反过来放弃反正也快守不住的可靠性，降低规模增长的能量需求？ 我认为是后者，虽然最近Hinton在牛津大学的演讲中虽然又提到了他的mortal computation，但它却还是选择放弃这条路，并期待原有路径下数字逻辑持续演进降低功耗，就是因为他始终舍不得可复制性，舍不得可复制性带来的Agent之间训练结果的共享（是不是很多人听完都没明白他在说啥？）。但是，教授，moore’law快死了，数字逻辑的功耗优化早已追不上Scaling-Law的节奏了。放弃吧 ：） 放弃数字电路，选择模拟甚至场电路来做计算，相同代价下至少可以保证Scaling-Law再增长3~4个零，真正的The One是不需要复制的。嗯，我很疯狂，我认为Altman筹到的七万亿应该投在模拟计算电路（IEEE754本身就是泥沼）以及大模型的算法革命（容错增强）。

如果Altman没有这么做，那就不需要像某些文章中的技术信仰派那么担忧，因为AI The One这条路始终存在一个小Trick，如果Scaling Law是对的，并且数字电路能够训练出The One的大模型，那么同样的代价用模拟电路是一定可以训练出The One X10 Scaling级别的The One大模型的，那前者就不是The One。

> Infinite

Agent及其可Infinite复制是另一条路，这也可以称为炼器之路。如果AI是保持为基于数字化的产品，那么起核心的价值就是基于前面提到的数字电路在满足了DRC、PVT约束后，可以产生的完完全全的完美的可复制品。与人生而为人，每个人都必须从小学习和成长，活到老卷到老不一样，某一个AI的产物一旦达到了某个能力水准，就可以批量性100%复制，任何问题，每一台AI的设备都能做出和第一台设备完全一样的能力和结果，那是完美无缺的Ctrl-C/Ctrl-V能力。如果要谈AI PMF，当前世界也只有基于人口基数进行有效复制并产生价值，才能撑得起期待中的的商业回报，才是AI在另一端的价值点。那古老的硅基生物，孙悟空，很早就通过喜闻乐见的传统故事为大家展示了正确的姿势。

![](../images/2e6342495f236b8b9da72ad66a20b55e.jpg)![](../images/2e6342495f236b8b9da72ad66a20b55e.jpg)

但是，什么AI PC就是扯淡的，心理疏导、排解寂寞，最多也就是对寺庙+教堂的一个缺乏信息安全的平替，唯一的感动也就是这个菩萨居然记得我半年前许的愿。AI要成为能每个人身边的生产娱乐工具，人的可用之器，必须达成物理世界（输入）->数字世界（输入）->数字世界（输出）->物理世界（输出）的闭环。PC的闭环是依赖物理的鼠标和显示器，Mobile的闭环是依赖物理的触屏本身，而当前号称的AI落地的场景（CV、聊天、调用app）都还是仅在数字世界打转，始终没有突破物理世界的阈值。嗯，也不能这么说，其实当前已经接近成功的AI落地，就是自动驾驶，这里的AI通过摄像头和轮胎最终完成了物理世界输入到物理世界输出的闭环及人与器的交互。车、脑机、机器人，都属于存在物理输入和输出接口，当前最接近可落地的AI形态（车其实就是没手只有4个轮子的机器人）。

这条路的瓶颈其实在于物理输入输出接口的标准化上，数字世界的AI Neural Networking就像灵魂，物理世界的车、机器人的接口就像身体，高矮不同、视力差异都会导致灵魂与身体适配的不适，得像数字世界克服DRC、PVT那样做好容错性（可复制性），即把AI的物理世界也标准化（可能是某种脑机？或者三指机械臂？），才是AI Infinite逐次演进的路。

不过，AI Infinite这条路也不用急，无论是谁先达成这个目标，最终完成的，只会是一个超级制造业的大国 ：） 无脑押注就好了。

> 当然，AI是The One还是Infinite，也许还没到可以分叉的时候 ：）

那再等等？

或者，美国人负责创造天网，中国人负责批量生产终结者，两家携手合作毁灭人类，哈哈，也不是不可能。

回到最后，如果要总结对AI的看法 ：）

**那有句话怎么说来的，“进窄门、走远路、见微光”，这条路本就是修行，不要急着得到，也不必急着放弃，世间总有能以终为始，保持战略定力、坚定长期坚持投入的人和公司的，额，你要说中国这样的公司不多，确实，至少我认为在知乎蛮受非议的某一家公司还算是吧 ：） 嗯，所以当前嘛，参考我的知乎头像，最重要还是锻炼身体，希望能战至终章吧，若到最后了还是不行，也只能说句“没关系的，都一样”了。**

---

*由知乎爬虫生成于 2026-02-01 15:39:00*
