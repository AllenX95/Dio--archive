# 我就蹭蹭热点（7）

> **类型**: 文章
> **作者**: Dio-晶
> **赞同**: 302
> **评论**: 18
> **时间**: 1655645050
> **原文**: [https://zhuanlan.zhihu.com/p/530927422](https://zhuanlan.zhihu.com/p/530927422)

---

最近的热点是啥呢？

![](../images/648500125a3e8911b2476b1890132712.jpg)![](../images/648500125a3e8911b2476b1890132712.jpg)

阿里的CIPU。

为了不和DPU、IPU重名，这起名字的煞费苦心不提也罢。

外行看热闹，内行看门道。

**一个关键值：RDMA延迟5.5us，很关键。**

我曾经在一个帖子谈过DPU落地困难。

<https://zhuanlan.zhihu.com/p/473102686>

**当今的数据中心，连RDMA都没有办法大规模部署！**

其实这句话说的不全对，例如AWS的nitro卡就通过SRD支持了数据中心的RDMA部署，但是为什么又说不行呢，那是因为AWS的RDMA延迟是15us。而在HPC超算中心的RDMA延迟是2us啊。

![](../images/63b6508bd5d9bbe50dc7ad164dc06bf0.jpg)![](../images/63b6508bd5d9bbe50dc7ad164dc06bf0.jpg)

---

所以这个热点就很有意思了，在2us to 15us之间，阿里给出了5.5us的一个答案。

![](../images/aef9b12c02b45096f326ff76bea0c54d.jpg)![](../images/aef9b12c02b45096f326ff76bea0c54d.jpg)

首先，为什么会有2us和15us两种值呢？

这源于数据中心的特征。

**数据中心本质上是依赖于在传统设施上叠加一层虚拟化来服务多用户的。**

粗俗的说就是每个嫖客都得带套，才不会得病。

这个分层在计算侧和网络侧都存在，如下图。

![](../images/890e24afa96f67769ace9b8c8ac0cedf.jpg)![](../images/890e24afa96f67769ace9b8c8ac0cedf.jpg)

上面两层正常来讲是客户可见可变，云服务商很难把控的。

而计算侧的虚拟化层（HYP）在几个软件巨头拉锯之后也趋于寡头化，主要是kvm和VMware，他们在云服务商手上也是很难去定制或者把控的。

那么，云服务商（AWS、阿里云等等）怎样表现自己的把控力、独特的竞争力和绩效呢？

**网络underlay层是云服务商在业务层面最大的抓手，并以此诞生了IPU/DPU。**

**每一家云服务商的underlay设计都是不一样的！都是定制化的！！并且不定期会发生变化（引入新功能）！！！**

RDMA本身是一个网络协议。

如果承载其packet的底层协议会经常变化，你该怎么办？

你一拍屁股就想到了，用一种可编程的network processor来处理它（软加速）。

对，这就是15us的来由。

而HPC超算中心，通常来讲是没有做虚拟化的，而且按系统整体交付没有兼容性需求，此时RDMA你可以完完全全用硬件的方式写一个，喏，这就是2us的来由（硬加速）。

---

那阿里为啥能做到5.5us这样一个中间值呢？

答案也不复杂，它用FPGA。

那意味着它即使把RDMA含虚拟化层全部写成hardware，他也可以定期重刷一个版本来适配云上的变化。

妈蛋。

**如果不考虑成本和能效，很有可能FPGA就是DPU真正可行的答案。**

![](../images/693bbfc9a2f8ac09b3a2e70428262ebe.jpg)![](../images/693bbfc9a2f8ac09b3a2e70428262ebe.jpg)

**题外话：broadcom收购VMware，想的是把计算侧的虚拟化和网络侧的虚拟化两个分离的层次融合到一起来进攻数据中心基础设施呢。**

---

*由知乎爬虫生成于 2026-02-01 15:39:00*
