# 从GPU谈异构（6）

> **类型**: 文章
> **作者**: Dio-晶
> **赞同**: 280
> **评论**: 29
> **时间**: 1623574634
> **原文**: [https://zhuanlan.zhihu.com/p/380180889](https://zhuanlan.zhihu.com/p/380180889)

---

端午假期深圳喜提龙舟水…………

原本想去钓个鱼，都只能取消

今天再一看，这天气预报是煞笔啊，全天都是晴天啊。唉，中国HPC崛起之路，吾辈任重道远。

～～～～

写了好几篇异构了，其实一直有一个深入灵魂的问题，我觉得需要重新提出来。

**什么是DSA，DSA应该做成什么样子?**

DSA：domain specific architecture

字面来讲就是领域特定架构，也就是针对某个应用领域而特别定制的某个架构。

那么，GPU是DSA吗？ 宽泛地讲，**是**。

那么，DSA是GPU吗？ 明显地说，**不是。**

所有针对某个特定领域的架构都是DSA！这里绝对不能被NVIDIA的GPU或者CUDA的长期暗示而把nvidia的东西和DSA等价，这个错误的概念一定要突破。

甚至于Nvidia现在主推GPGPU，这其实严格来讲都不是DSA了，而是multi-domain specific architecture。

对啦，所谓domain，其实存在一个度的概念，并不是非黑即白。intel的CPU的也会分很多的bin（分类啦），这些x86 xeon CPU在通过某种策略细分之后，也变成了某种many-domain specific architecture了。

**水无形而常有形。** 受过中国文化教育的我想都很容易理解这个问题。

![](../images/c887ccf2c0a201a57edf580b3da7c48d.jpg)![](../images/c887ccf2c0a201a57edf580b3da7c48d.jpg)

所以**domain**其实是是一个度的范畴。

nvidia的A100 GPU是AI与HPC的DSA

google的TPU1/2/3/4是AI的DSA

fujitsu的Afx64 CPU也是HPC的DSA

apple A14 CPU的为AI加速AMX coprocessor是不是DSA呢？

如果不是，AMX算啥呢？ 通用处理器的一次出轨? 如果是，那为啥CPU都是DSA了，还做GPU干嘛呢?

很混乱，是的这个世界的真相就是在混沌之中选择某种平衡而已。

**以Patterson大神的DSA黄金十年宣言为起点**：

![](../images/fea984b925607b6c2a19bed584994929.jpg)![](../images/fea984b925607b6c2a19bed584994929.jpg)

专用处理器的ISA擅长在domain刚起步的初期就迅速起步（没有历史负担），通过超越通用处理器的PPA收益和时间，换取在超越其定制化开销之外的额外收益，快速崛起。如果这个市场空间足够诱人，并且存在赢者通吃的可能性时，那是一定要第一时间投资专用处理器去抢头啖汤的。

通用处理器的ISA发展相对较慢，但随着低垂的苹果都逐渐被摘掉，通用处理器也开始逐渐倾向于获取domain特征的收益，通过ISA的可定制化扩展在通用的基础上面对专用领域也能获得额外性能收益，intel的AVX、AMX都是这种倾向的产物。不过通用处理器这种扩展相对会比较慎重，毕竟这些专用化定制可能成为未来数十年的负担。但如果专用处理器的定制开销＞定制收益+时间时，通用处理器就摘果子了。

随着时间的发展，当专用处理器的时间领先收益逐渐缩小的时候，专用处理器就会想办法通过寻找新的空间来扩大其定制收益，保证即使放弃时间收益缩，依旧定制收益＞定制代价。嗯，nvidia增加tensorcore就是这样啦。

这个世界，如循环，生生不灭。

![](../images/6b1b8f64ebc6315d3b8d0a3bebbb33a7.jpg)![](../images/6b1b8f64ebc6315d3b8d0a3bebbb33a7.jpg)

**延伸到第二个话题，如何设计一个恰当的DSA?**

首先当然需要看的是domain是什么，即是否存在一个合适的专用的市场空间，直白的说，就是：**钱够不够?** 回头看看AI DSA这些年的热火，就是个正面的例子。作为对比我举个反面的例子，学计算机体系结构应该都知道anton处理器，一个顶级大富豪私人出资专门做MD计算的DSA专用处理器，最终只搭建了两个实验性集群，其专用性很高，只能做MD，针对的主要是新型制药。如果想想黄峥、XXX等等大佬的急流勇退和转向，不难理解，anton和徐福东渡的目的有着说不清道不明的关系(ಡωಡ) 。anton也不能说失败吧，主要是生物化学这个领域，那是神的禁区，即使anton提升了百倍性能（太专用也做不了其他啥），离突破那层纸，最终还是咫尺天涯。

插个广告：诸位中国新兴富豪们，要不过两年聘我再试试? 只要钱给够，啥姿势我都会（真的有机会）。

做DSA，**domain这个东西其实是个变量**（即使AI亦是），那么如何基于这个变量，选择最佳满足市场需求，具有最佳竞争力的DSA架构就是核心竞争力了。

是时候掏出裤裆里最大的宝贝了。

![](../images/6608fa92f90eb7923da93e55e6fa5c80.jpg)![](../images/6608fa92f90eb7923da93e55e6fa5c80.jpg)

这张图我觉得价值百万(ಡωಡ)

不过我就是觉得应该是没多少人能懂，就算能看懂呢，可能大家都价值观又有差异，或者对我想延伸的更深层得失。我简单说明一下其中的几个转折。

首先，无论是向左向右，并不直接映射为DSA domain的大小，但会影响从芯片内部到芯片之间甚至集群结构上DSA的domain size，甚至进而影响我在上一篇提到的异构编程框架的变化。

<https://zhuanlan.zhihu.com/p/378279761>

其次，图中最简单的分类可以分成：共享load/store、共享frontend（ISA）、共享MEM几个边界，这些边界最终会形成对domain大小及PPA收益高低的综合得失，这很重要，但是又难以一言而概之。其下还涉及到ARM/X86/RISC-V/完全自定义ISA/FIX FUNC等实际可选空间。

简单说几个点。

在ARM和X86上做DSA，正常来讲，都是采用共享CPU前端和后端的策略，AVX、SVE都是如此，首先这两套ISA本身就是相当复杂的体系，在此基础上如果要独立前后端都相当于是对已有资源的浪费，并且X86和ARM都能支持richOS，只有复用前后端才能保证richOS支撑的完备度（为一个DSA放弃RichOS多可惜）。

而在RISC-V做DSA，其特征就是复用前端，但基本上没人复用后端，都会采用独立LOAD/store及BUFFER，主要原因就是RISC-V就47条基础指令，带来了前端设计的极其简洁，而SIMD类指令复杂度是相当高的，例如ARM SVE就有差不多800条，47+800还算RISC-V嘛? 所以为了保证RV已有的收益，RV的DSA几乎一定是通过宏指令方式扩展，即少数的具有巨大抽象意义或能够二次展开的巨大化指令来扩展。这也是RISC-V的SIMD ISA被废弃，而采用vector扩展ISA方式的原因。

RISC-V的计算类DSA扩展是vector向量机结构（源自CRAY），不是SIMD，这其中有巨大的差异很多人都没有意识到。曾经有位朋友问我，RV的vector扩展能否做到共享load/store，我的回答是不行，因为向量机的特征会把让原CORE上运行的OS失去很多必备特征。

正如我在上图对richOS画了一个边界（其实不仅仅是richOS，RTOS亦是），这个点也许存在一定的争议啊。因为要满足一个OS的基本运行，context switch/中断及异常的处理是必备特征，这种OS必备特征所能忍受的延迟，没有书本有描述，但以我曾经参与过小型机开发经验来讲，大致是在800ns～1us之间，而RV vector这类的行为，在独立的loop和load下，其行为完全可以耦合住CPU本体导致超过1us都无法进行中断响应等事务。

需要额外提及的是图中loose DSA部分，这里的架构空间有很大一块是留给FIX FUNCTION的，某些算法如果已经存在较多的确定性，即复杂行为已经被确定性抽象表达，就像吃饭、睡觉这种高度抽象化有没有歧义的，这类DSA被称作HAC，CPU可以以produce-consumer方式按COMMAND编码将task下放给HAC DSA执行，这类DSA可以简称为CMD ISA，收益大，编程简单，参考intel的QAT设备，传统的IPSEC、ZIP等均在于此。

**关于ISA部分简单总结：**拿掉可以固化为FIX FUNC的CMD ISA；如果DSA要复用ARM/X86 ISA的前端，尽量采用复杂的细粒度指令扩展方式；如果DSA复用RISC-V ISA的前端，尽量采用粗粒度的指令扩展方式；如果不复用ARM/X86/RISC-V的ISA前端，那么自定义ISA你细粒度、粗粒度咋玩都行，但是大概率呢，你会丧失掉图灵完备性，导致无法运行通用OS而需要一个HOST来掌控你的人生，从此风雨飘摇处处寻找一个可以依赖的肩膀。 这事也不用特别争辩，从理论上达成图灵完备度并不难，但是要做到图灵生态完备，当今世上，主流也就ARM、X86和RISC-V。

前面光讲了独立的Load/store或独立的ISA得放弃OS完备性的坏处，那么没有好处吗？ 当然有，否则为啥业界大量的DSA都是独立ISA和独立load/store为主。这其实是从我们注意到一个domain足够大的时候就具有的倾向性了。

问题是：**加速这个domain所需要的compute patten是多大?**

对X86 AVX如果足够熟悉的人应该知道我想表达什么。假设用AVX来计算AI的一个图像的卷积，这种很简单的遍历性的计算模式，需要非常多细粒度AVX指令来表达，如果没有采用MKL这类承载了优化经验的库帮助，执行效率会很低。而基于卷积核和图的大小，你还需要进一步按64B CACHELINE及L1 cache size来做tiling。其中最大的约束莫过于64B cacheline size了，这种通用计算下折中而得的数据尺寸，是不符合大多数大颗粒计算任务的bandwidth诉求的。所以这里反向也说明一下X86/ARM做DSA的瓶颈：**是的，获得OS图灵完备度代价，就是放弃DSA大粒度的计算效率。你需要在其中寻求一种平衡。**

**这里的总结是：如果倾向于极致专用的domain，并且DSA计算粒度中等偏上，最好选择独立做后端load/store/buffer结构；如果倾向于面对的domain更广，DSA计算粒度中偏小，在传统CPU内扩展DSA并复用CPU的存储系统更加便捷。**

![](../images/74ac386ca49475a20241ee30bd0c83da.jpg)![](data:image/svg+xml;utf8,<svg%20xmlns='http://www.w3.org/2000/svg'%20width='349'%20height='129'></svg>)

大致的情况是这个样子，其实具体需要思考的细节还蛮多的，我放个假也不容易，没法像写论文一样再展开。但是呢，不得不说的是，当下体系结构中**做DSA的顶尖大师们**的思路，则都是意识到上面所讲的取舍之后，选择在边界上反复横跳(ಡωಡ) ， 就是那种，在做死的边缘反复试探的赶脚来获取最大的利益。

![](../images/4daaef29f0a4d9b54639d86568732d90.jpg)![](../images/4daaef29f0a4d9b54639d86568732d90.jpg)

例如ARM的矩阵计算指令SME，那基本上是踩在context switch的悬崖边上的杰作，能算矩阵也能支持richOS。但是apple的AMX却更加精彩，表面上它是在ARM ISA上做的扩展，但是它硬是突破了context switch的边界做到了宏指令级别获得更巨大的PPA收益，虽然依旧复用原CPU的cache，但有独立的跨越cacheline尺度的读写能力。要问为啥apple可以这样横跳? 很简单也不简单，apple有两个大核，其中一个核做AMX计算时，另一个核负责响应中断或context switch，OS调度把握在apple手上，还是得服。

再说一句AI吧。

AI训练这个domain，从当下来看，几乎不存在基于X86/ARM扩展DSA的可能性，因为这个领域够值钱，数据就是财富不用多说，基本上算得上数的大企业，一定需要自建一个符合自身利益的训练集群来挖掘数据中最大的利益（这其实是某种意义的洗钱，懂得自然懂），这事毋庸置疑，其训练的网络尺寸也会日益增长，即这是一个增量市场，而训练本身是基于计算可微的一种复杂框架建立的，其计算颗粒度足够大，做独立DSA能够从中获取巨大的PPA收益。

但AI推理这个领域就有意思了，其实说白了不就是跑一个流式计算嘛，有多复杂? CPU也已经卖了，intel的sapphire rapids之后的XEON CPU都支持了类似tensor core的AMX DSA，CPU比GPU可便宜多了，而且还是必备品。嗯，这就是一个典型通用处理器扩展DSA逐渐超越专用处理器DSA的例子。不过，讲这么多还是为了引出跨边界横跳的Google TPU4i（i是推理版本），为啥Google还依旧在使用TPU而不用CPU做推理? 也很简单，G家的推理业务以NLP为主，都是云端的大网络级别推理，此时降低每次推理的成本就变成了最大的诉求，大网络又要低成本，变相就是这个domain特别专但是又有钱咯呗。

**所以呢，这个世界是复杂而变化的，DSA亦是。**

端午节被错误的天气预报困在家里，回首一看，居然写了这么多内容…………

![](../images/71fbf57f41b056fd41248a23495e2e8e.jpg)![](../images/71fbf57f41b056fd41248a23495e2e8e.jpg)

---

*由知乎爬虫生成于 2026-02-01 15:39:00*
