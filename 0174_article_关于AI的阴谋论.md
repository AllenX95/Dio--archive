# 关于AI的阴谋论

> **类型**: 文章
> **作者**: Dio-晶
> **赞同**: 104
> **评论**: 24
> **时间**: 1619836375
> **原文**: [https://zhuanlan.zhihu.com/p/369220447](https://zhuanlan.zhihu.com/p/369220447)

---

最近，与一位朋友一杯咖啡吸收宇宙能量之际，听他作为当事人讲述了当年alexnet是如何代表deep learning方向超越贝叶斯等其他方向，成为当前AI发展的主流方向的。

从故事性来讲，其中充满了各种际遇巧合，XXX和YYY恰巧的相遇，YYY和ZZZ恰好的互补，犹如苹果砸在了牛顿的头上，天时地利人和，缺一不可，都是上帝的安排。

![](../images/47ec381aab29f1e94bc4c584e1f10294.jpg)![](../images/47ec381aab29f1e94bc4c584e1f10294.jpg)

但要认真一盘，你会发现核心的原因除开数据爆炸带来的大数据背景，就是硬件算力(GPU)恰好突破了某个阈值，让其计算形态能够获得非常好的加速，犹如催化剂一般带来了巨大的飞轮效应，后面的大家都看到了。其实，没有alexNet，也终会出现kaiserNet。

算法有很多，各种流派原本在数学上是华山论剑，各领风骚的。但硬件，是的，硬件插了一脚，硬件不讲道理且盲目的对某种算法的偏爱，却决定了AI算法的发展路径！ 这真不对！

麻痹，做硬件不能博爱一点吗？

![](../images/8f978f46c9d7ede38689e78a2ce44d90.gif)![](data:image/svg+xml;utf8,<svg%20xmlns='http://www.w3.org/2000/svg'%20width='240'%20height='144'></svg>)

我很久以前有一个预言，TPU的存在会影响AI网络的算法演进方向。

世上根本就没有达尔文，就是外星人喜欢我们的样子！

让我们来看看AI发展成什么样子了。

**deep learning早已不再deep，它应该换一个名字叫wide learning，对，宽度学习(TPU的互联形态做不了高连接的模型并行，模型深度大概率受限于单个TPU尺寸，然后疯狂expert级数据并行)。**

**深度受限下，weight为王的时代又转变成embeding为王，诸事不决查个字典，网络只算index，存算变得分离。曾经火热过的PIM架构亦将难以跟随(做DL存内计算方向的小伙伴们你们还好吗？你们没有错，错的是这个时代)。**

**巨型矩阵计算依旧大行其道，即使其中具有巨量的sparse，却没人愿意HASH(TPU的systolic做不了sparse或hash)。**

**embedding线性存储具体大量信息冗余，越来越大却没人愿意优化其存放格式(我猜测的是TPU的load/store就只会direct access memory)。**

**一直觉得很有意思的GNN网络根本发展不起来(不匹配TPU硬件特征)。**

以上的所有的进步，在我看来，都不是在数学上获得了算法的升级或者突破。我提过一个问题，我有我的答案，但是一切无能为力。

[transformer所使用的embedding table，请问有没有人觉得是有问题的?](https://www.zhihu.com/question/457370601)

回到基本想一想吧，如果我们真的想要让AI变成强人工智能，那，人是怎样处理NLP的? 人确实会建立一个巨embedding table，这条路是对的，但人记忆语言更多是在lookup的时候构造复杂的映射关系，而不是一个线性表啊。

目前的现在，这，纯粹就是Google巨量的暴力的矩阵运算单元硬件形态驱使而成。Google拥有天量的研究员，Google有钱定制TPU并且大规模量产给内部使用，研究员TPU算力管够。

那结果还用说吗？ AI的未来注定在TPU affinite的道路上一路狂奔……

这是对的吗？ 这是AI的硬件形态导致的算法和算力的独裁啊！

继续这么发展下去，你要么做和TPU近乎一样的硬件，要么购买google的SaaS。春天将只剩下一种颜色。

有没有人想要来造反呢? 让我们AI的未来，更多的是数学驱动！算法驱动！

王侯将相宁有种乎。

![](../images/6a4869240532a5b3ac06509f5132b571.jpg)![](../images/6a4869240532a5b3ac06509f5132b571.jpg)

---

*由知乎爬虫生成于 2026-02-01 15:39:00*
